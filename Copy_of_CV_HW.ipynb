{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Copy of CV HW",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NosenkoArtem/MADMO/blob/master/Copy_of_CV_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Ag-Jk1wzlIox",
        "colab_type": "text"
      },
      "source": [
        "# Homework 2.2: The Quest For A Better Network\n",
        "\n",
        "In this assignment you will build a monster network to solve Tiny ImageNet image classification.\n",
        "\n",
        "This notebook is intended as a sequel to seminar 3, please give it a try if you haven't done so yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eTm51ELlIoz",
        "colab_type": "text"
      },
      "source": [
        "(please read it at least diagonally)\n",
        "\n",
        "* The ultimate quest is to create a network that has as high __accuracy__ as you can push it.\n",
        "* There is a __mini-report__ at the end that you will have to fill in. We recommend reading it first and filling it while you iterate.\n",
        " \n",
        "## Grading\n",
        "* starting at zero points\n",
        "* +20% for describing your iteration path in a report below.\n",
        "* +20% for building a network that gets above 20% accuracy\n",
        "* +10% for beating each of these milestones on __TEST__ dataset:\n",
        "    * 25% (50% points)\n",
        "    * 30% (60% points)\n",
        "    * 32.5% (70% points)\n",
        "    * 35% (80% points)\n",
        "    * 37.5% (90% points)\n",
        "    * 40% (full points)\n",
        "    \n",
        "## Restrictions\n",
        "* Please do NOT use pre-trained networks for this assignment until you reach 40%.\n",
        " * In other words, base milestones must be beaten without pre-trained nets (and such net must be present in the anytask atttachments). After that, you can use whatever you want.\n",
        "* you __can't__ do anything with validation data apart from running the evaluation procedure. Please, split train images on train and validation parts\n",
        "\n",
        "## Tips on what can be done:\n",
        "\n",
        "\n",
        " * __Network size__\n",
        "   * MOAR neurons, \n",
        "   * MOAR layers, ([torch.nn docs](http://pytorch.org/docs/master/nn.html))\n",
        "\n",
        "   * Nonlinearities in the hidden layers\n",
        "     * tanh, relu, leaky relu, etc\n",
        "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
        "\n",
        "   * Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn!\n",
        "\n",
        "\n",
        "### The main rule of prototyping: one change at a time\n",
        "   * By now you probably have several ideas on what to change. By all means, try them out! But there's a catch: __never test several new things at once__.\n",
        "\n",
        "\n",
        "### Optimization\n",
        "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
        "   * Some networks converge over 5 epochs, others - over 500.\n",
        "   * Way to go: stop when validation score is 10 iterations past maximum\n",
        "   * You should certainly use adaptive optimizers\n",
        "     * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
        "     * Converge faster and sometimes reach better optima\n",
        "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
        "   * __BatchNormalization__ (nn.BatchNorm2d) for the win!\n",
        "     * Sometimes more batch normalization is better.\n",
        "   * __Regularize__ to prevent overfitting\n",
        "     * Add some L2 weight norm to the loss function, PyTorch will do the rest\n",
        "       * Can be done manually or like [this](https://discuss.pytorch.org/t/simple-l2-regularization/139/2).\n",
        "     * Dropout (`nn.Dropout`) - to prevent overfitting\n",
        "       * Don't overdo it. Check if it actually makes your network better\n",
        "   \n",
        "### Convolution architectures\n",
        "   * This task __can__ be solved by a sequence of convolutions and poolings with batch_norm and ReLU seasoning, but you shouldn't necessarily stop there.\n",
        "   * [Inception family](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/), [ResNet family](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035?gi=9018057983ca), [Densely-connected convolutions (exotic)](https://arxiv.org/abs/1608.06993), [Capsule networks (exotic)](https://arxiv.org/abs/1710.09829)\n",
        "   * Please do try a few simple architectures before you go for resnet-152.\n",
        "   * Warning! Training convolutional networks can take long without GPU. That's okay.\n",
        "     * If you are CPU-only, we still recomment that you try a simple convolutional architecture\n",
        "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
        "     * Make reasonable layer size estimates. A 128-neuron first convolution is likely an overkill.\n",
        "     * __To reduce computation__ time by a factor in exchange for some accuracy drop, try using __stride__ parameter. A stride=2 convolution should take roughly 1/4 of the default (stride=1) one.\n",
        " \n",
        "   \n",
        "### Data augmemntation\n",
        "   * getting 5x as large dataset for free is a great \n",
        "     * Zoom-in+slice = move\n",
        "     * Rotate+zoom(to remove black stripes)\n",
        "     * Add Noize (gaussian or bernoulli)\n",
        "   * Simple way to do that (if you have PIL/Image): \n",
        "     * ```from scipy.misc import imrotate,imresize```\n",
        "     * and a few slicing\n",
        "     * Other cool libraries: cv2, skimake, PIL/Pillow\n",
        "   * A more advanced way is to use torchvision transforms:\n",
        "    ```\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    trainset = torchvision.datasets.ImageFolder(root=path_to_tiny_imagenet, train=True, download=True, transform=transform_train)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "    ```\n",
        "   * Or use this tool from Keras (requires theano/tensorflow): [tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), [docs](https://keras.io/preprocessing/image/)\n",
        "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them.\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VolwcDVlIo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn as nn\n",
        "torch.manual_seed(42)\n",
        "\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxL-9xiWlIo6",
        "colab_type": "text"
      },
      "source": [
        "So let's load some data..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5Bzv4SZlIo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/spring2019/homework02/tiny_img.py\n",
        "from tiny_img import download_tinyImg200\n",
        "data_path = '.'\n",
        "download_tinyImg200(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU93qHlnlIo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/train', transform=torchvision.transforms.ToTensor())\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [80000, 20000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCt6SLvolIpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = sorted(np.unique([data[1] for data in train_dataset]))\n",
        "n_classes = len(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Baje4Ve0lIpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(model, X_batch, y_batch, C=0, criterion=nn.L1Loss(size_average=False)):\n",
        "    logits = model.cuda()(X_batch.cuda())\n",
        "    \n",
        "    loss = <YOUR CODE>\n",
        "    \n",
        "    # Here we add some regularization\n",
        "    if C:\n",
        "        reg_loss = 0\n",
        "        for param in model.parameters():\n",
        "            reg_loss += criterion(param, torch.zeros(param.shape).cuda())\n",
        "        \n",
        "        loss += C * reg_loss\n",
        "    \n",
        "    return logits, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prbEa8tdlIpG",
        "colab_type": "text"
      },
      "source": [
        "Please, define your model below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JSX7INplIpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = <YOUR CODE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VNjc4rKlIpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = <YOUR CODE>\n",
        "\n",
        "train_batch_gen = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "val_batch_gen = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVPmTo5llIpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "from collections import namedtuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "Logger = namedtuple('Logger', ['train_loss_', 'train_accuracy_', 'val_loss_', 'val_accuracy_'])\n",
        "\n",
        "\n",
        "def get_logger():\n",
        "    return Logger(*[[] for _ in range(4)])\n",
        "\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    return np.mean((y_true.cpu() == y_pred.cpu()).numpy())\n",
        "\n",
        "\n",
        "def evaluate_model(model, val_batch_gen, C=0, logs=None):\n",
        "    if not logs:\n",
        "        logs = get_logger()\n",
        "    \n",
        "    model.train(False)\n",
        "    for X_batch, y_batch in val_batch_gen:\n",
        "        logits, loss = compute_loss(model, X_batch, y_batch, C)\n",
        "        y_pred = logits.max(1)[1].data\n",
        "        logs.val_loss_.append(loss.cpu().data.numpy())\n",
        "        logs.val_accuracy_.append(compute_accuracy(y_batch, y_pred))\n",
        "        \n",
        "    return logs\n",
        "\n",
        "\n",
        "def train_model(model, train_batch_gen, val_batch_gen, n_epochs=50, plot_graph=True, C=0, **optimizer_params):\n",
        "    opt = torch.optim.Adam(model.parameters(), **optimizer_params)\n",
        "    logs = get_logger()\n",
        "    \n",
        "    for epoch in tqdm(range(n_epochs)):\n",
        "        epoch_logs = get_logger()\n",
        "        \n",
        "        model.train(True)\n",
        "        for X_batch, y_batch in train_batch_gen:\n",
        "            logits, loss = compute_loss(model, X_batch, y_batch, C)\n",
        "            y_pred = logits.max(1)[1].data\n",
        "\n",
        "            <YOUR CODE> # Here you need to make a gradient step\n",
        "\n",
        "            epoch_logs.train_loss_.append(loss.cpu().data.numpy())\n",
        "            epoch_logs.train_accuracy_.append(compute_accuracy(y_batch, y_pred))\n",
        "        \n",
        "        evaluate_model(model, val_batch_gen, C, epoch_logs)\n",
        "            \n",
        "        for attr in dir(epoch_logs):\n",
        "            if attr.endswith('_') and not attr.endswith('__'):\n",
        "                getattr(logs, attr).append(np.mean(getattr(epoch_logs, attr)))\n",
        "    \n",
        "        if plot_graph:\n",
        "            clear_output()\n",
        "            plt.figure(figsize=(15, 5))\n",
        "            grid = np.arange(1, epoch + 2)\n",
        "            \n",
        "            fig = plt.subplot(121)\n",
        "            fig.set_title('Loss in dependency on number of epochs')\n",
        "            fig.plot(grid, logs.train_loss_, label='train')\n",
        "            fig.plot(grid, logs.val_loss_, label='val')\n",
        "            fig.set_xlabel('epoch')\n",
        "            fig.set_ylabel('loss')\n",
        "            fig.grid()\n",
        "            \n",
        "            fig = plt.subplot(122)\n",
        "            fig.set_title('Accuracy in dependency on number of epochs')\n",
        "            fig.plot(grid, logs.train_accuracy_, label='train')\n",
        "            fig.plot(grid, logs.val_accuracy_, label='val')\n",
        "            fig.set_xlabel('epoch')\n",
        "            fig.set_ylabel('accuracy')\n",
        "            fig.grid()\n",
        "            \n",
        "            plt.show()\n",
        "            \n",
        "    return logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39fjEFdFlIpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logs = train_model(model, train_batch_gen, val_batch_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez2e8BOOlIpQ",
        "colab_type": "text"
      },
      "source": [
        "This is the time to add some regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u15nfEOblIpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = <YOUR CODE>\n",
        "logs = train_model(model, train_batch_gen, val_batch_gen, C=<YOUR CODE>)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olgg_7iCpkJ5",
        "colab_type": "text"
      },
      "source": [
        "The interface provided below applies transformations to the input data while training your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kEi2RuolIpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from copy import deepcopy\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    <YOUR CODE>\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/train/', transform=transform)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [80000, 20000])\n",
        "\n",
        "train_batch_gen = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    <YOUR CODE>\n",
        "])\n",
        "\n",
        "val_dataset = deepcopy(val_dataset)\n",
        "val_dataset.dataset.transform = test_transform\n",
        "val_batch_gen = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9MEk2ghlIpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = <YOUR CODE>\n",
        "logs = <YOUR CODE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJiZKDxmlIpX",
        "colab_type": "text"
      },
      "source": [
        "When everything is done, please calculate accuracy on `tiny-imagenet-200/val`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhDlhJTdlIpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from skimage.io import imread\n",
        "\n",
        "\n",
        "class TestLoader(torchvision.datasets.DatasetFolder):\n",
        "    def __init__(self, root, annotation_file, class_to_label, transform=None):\n",
        "        self.annotation = pd.read_csv(annotation_file, sep='\\t', header=None)\n",
        "        self.class_to_label = class_to_label\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.annotation.shape[0]\n",
        "  \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root, self._get_name(idx))\n",
        "        image = imread(img_path)\n",
        "        \n",
        "        if image.ndim != 3:\n",
        "            image = np.concatenate([image[:, :, np.newaxis] for _ in range(3)], axis=-1)\n",
        "    \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "    \n",
        "        return image, self.class_to_label[self._get_class(idx)]\n",
        "    \n",
        "    def _get_class(self, idx):\n",
        "        return self.annotation.iloc[idx, 1]\n",
        "    \n",
        "    def _get_name(self, idx):\n",
        "        return self.annotation.iloc[idx, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1K0ssK2lIpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = TestLoader('tiny-imagenet-200/val/images/','tiny-imagenet-200/val/val_annotations.txt',\n",
        "                          train_dataset.dataset.class_to_idx, transform=test_transform)\n",
        "test_batch_gen = torch.utils.data.DataLoader(test_dataset, batch_size=50, shuffle=True, num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgQT8XxblIpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_logs = evaluate_model(model, test_batch_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzbXguvQlIpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_accuracy = np.mean(test_logs.val_accuracy_)\n",
        "\n",
        "print(\"Final results:\")\n",
        "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
        "    test_accuracy * 100))\n",
        "\n",
        "if test_accuracy * 100 > 40:\n",
        "    print(\"Achievement unlocked: 110lvl Warlock!\")\n",
        "elif test_accuracy * 100 > 35:\n",
        "    print(\"Achievement unlocked: 80lvl Warlock!\")\n",
        "elif test_accuracy * 100 > 30:\n",
        "    print(\"Achievement unlocked: 70lvl Warlock!\")\n",
        "elif test_accuracy * 100 > 25:\n",
        "    print(\"Achievement unlocked: 60lvl Warlock!\")\n",
        "else:\n",
        "    print(\"We need more magic! Follow instructons below\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu81KiGClIpg",
        "colab_type": "text"
      },
      "source": [
        "# Report\n",
        "\n",
        "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
        "* the idea;\n",
        "* brief history of tweaks and improvements;\n",
        "* what is the final architecture and why?\n",
        "* what is the training method and, again, why?\n",
        "* Any regularizations and other techniques applied and their effects;\n",
        "\n",
        "\n",
        "There is no need to write strict mathematical proofs (unless you want to).\n",
        " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
        " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
        " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Om31qjlIph",
        "colab_type": "text"
      },
      "source": [
        "### Hi, my name is `___ ___`, and here's my story\n",
        "\n",
        "A long time ago in a galaxy far far away, when it was still more than an hour before the deadline, i got an idea:\n",
        "\n",
        "##### I gonna build a neural network, that\n",
        "* brief text on what was\n",
        "* the original idea\n",
        "* and why it was so\n",
        "\n",
        "How could i be so naive?!\n",
        "\n",
        "##### One day, with no signs of warning,\n",
        "This thing has finally converged and\n",
        "* Some explaination about what were the results,\n",
        "* what worked and what didn't\n",
        "* most importantly - what next steps were taken, if any\n",
        "* and what were their respective outcomes\n",
        "\n",
        "##### Finally, after __  iterations, __ mugs of [tea/coffee]\n",
        "* what was the final architecture\n",
        "* as well as training method and tricks\n",
        "\n",
        "That, having wasted ____ [minutes, hours or days] of my life training, got\n",
        "\n",
        "* accuracy on training: __\n",
        "* accuracy on validation: __\n",
        "* accuracy on test: __\n",
        "\n",
        "\n",
        "[an optional afterword and mortal curses on assignment authors]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV0N3Zz_lIph",
        "colab_type": "text"
      },
      "source": [
        "### Short report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkz-FP_olIpk",
        "colab_type": "text"
      },
      "source": [
        "\\<YOUR CODE\\>"
      ]
    }
  ]
}